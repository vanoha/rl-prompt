# Data
dataset: "./data"
dataset_seed: null
# Reward
style_tokenizer: "bert-base-uncased"
lower_outputs: true
control_output_length: true
# Single Prompt Model
prompt_length: 5
prompt_train_batch_size: 8
prompt_infer_batch_size: 16
# LM Adaptor Model
logit_bias: -10
# Trainer
train_batch_size: 2
max_train_steps: 12000
train_shuffle: false
eval_batch_size: 16
eval_steps: 50
save_steps: 100
learning_rate: 5e-5
random_seed: null
